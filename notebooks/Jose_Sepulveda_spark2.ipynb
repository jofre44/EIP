{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9fe479b",
   "metadata": {},
   "source": [
    "Act 8. Programación Python para Big Data\n",
    "\n",
    "Alumno: Jose Sepulveda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065bb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ceef37",
   "metadata": {},
   "source": [
    "## Actividad \n",
    "\n",
    "El/la alumno/a deberá entregar un archivo .ipynb como actividad con las\n",
    "siguientes partes:\n",
    "\n",
    "**Actividad 1** (2,0 ptos)\n",
    "\n",
    "Transformar en vector los datos numéricos usando\n",
    "VectorAssembler\n",
    "\n",
    "**Respuesta:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b3c4879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/09/23 12:58:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+--------------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|      numeric_fields|\n",
      "+------------+-----------+------------+-----------+-------+--------------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|[5.09999990463256...|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|[4.90000009536743...|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|[4.69999980926513...|\n",
      "|         4.6|        3.1|         1.5|        0.2| setosa|[4.59999990463256...|\n",
      "|         5.0|        3.6|         1.4|        0.2| setosa|[5.0,3.5999999046...|\n",
      "|         5.4|        3.9|         1.7|        0.4| setosa|[5.40000009536743...|\n",
      "|         4.6|        3.4|         1.4|        0.3| setosa|[4.59999990463256...|\n",
      "|         5.0|        3.4|         1.5|        0.2| setosa|[5.0,3.4000000953...|\n",
      "|         4.4|        2.9|         1.4|        0.2| setosa|[4.40000009536743...|\n",
      "|         4.9|        3.1|         1.5|        0.1| setosa|[4.90000009536743...|\n",
      "+------------+-----------+------------+-----------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importamos funciones necesarias\n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.sql.functions import col\n",
    "# creamos sesion\n",
    "spark = SparkSession.builder.appName('titanic').getOrCreate() \n",
    "\n",
    "# cargamos datos\n",
    "df = spark.read.csv('data/iris.csv', header = True).cache()\n",
    "\n",
    "# se cambia el tipo de datos de las variables\n",
    "df = df.select(col('sepal_length').cast('float'), \n",
    "               col('sepal_width').cast('float'), \n",
    "               col('petal_length').cast('float'), \n",
    "               col('petal_width').cast('float'), \n",
    "               col('species') )\n",
    "\n",
    "# seleccion de field numericos\n",
    "required_features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'] \n",
    "\n",
    "# creamos instancia\n",
    "assembler = VectorAssembler(inputCols=required_features, outputCol='numeric_fields') \n",
    "\n",
    "# transmormamos data\n",
    "df_trans = assembler.transform(df) \n",
    "\n",
    "df_trans.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798fb11d",
   "metadata": {},
   "source": [
    "**Actividad 2** (2,0 ptos)\n",
    "\n",
    "Transformar los datos correspondientes a la columna especies\n",
    "usando StringIndexer\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3c4a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+--------------------+----+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|      numeric_fields|type|\n",
      "+------------+-----------+------------+-----------+--------------------+----+\n",
      "|         5.1|        3.5|         1.4|        0.2|[5.09999990463256...| 0.0|\n",
      "|         4.9|        3.0|         1.4|        0.2|[4.90000009536743...| 0.0|\n",
      "|         4.7|        3.2|         1.3|        0.2|[4.69999980926513...| 0.0|\n",
      "|         4.6|        3.1|         1.5|        0.2|[4.59999990463256...| 0.0|\n",
      "|         5.0|        3.6|         1.4|        0.2|[5.0,3.5999999046...| 0.0|\n",
      "|         5.4|        3.9|         1.7|        0.4|[5.40000009536743...| 0.0|\n",
      "|         4.6|        3.4|         1.4|        0.3|[4.59999990463256...| 0.0|\n",
      "|         5.0|        3.4|         1.5|        0.2|[5.0,3.4000000953...| 0.0|\n",
      "|         4.4|        2.9|         1.4|        0.2|[4.40000009536743...| 0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|[4.90000009536743...| 0.0|\n",
      "+------------+-----------+------------+-----------+--------------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importamos funciones necesarias\n",
    "from pyspark.ml.feature import StringIndexer \n",
    "\n",
    "# transformamos variable string\n",
    "df_trans = StringIndexer(inputCol='species', outputCol='type', handleInvalid='keep')\\\n",
    "    .fit(df).transform(df_trans) \n",
    "\n",
    "# eliminamos variable species \n",
    "df_trans = df_trans.drop('species')\n",
    "df_trans.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574f5af",
   "metadata": {},
   "source": [
    "**Actividad 3** (2,0 ptos)\n",
    "\n",
    "Calcular la predicción y la precisión del modelo empleando Decision Tree Classifier\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ff094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision del test =  0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# importamos funciones necesarias\n",
    "from pyspark.ml.classification import DecisionTreeClassifier \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# creamos train and test df\n",
    "(df_train, df_test) = df_trans.randomSplit([0.8,0.2],44)\n",
    "\n",
    "# inicializamos modele\n",
    "dt = DecisionTreeClassifier(labelCol='type', featuresCol='numeric_fields', maxDepth=5)\n",
    "\n",
    "# entrenamos model\n",
    "model = dt.fit(df_train)\n",
    "\n",
    "# hacemos prediccion\n",
    "predictions = model.transform(df_test)\n",
    "\n",
    "# calculamos metrica\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='type', \n",
    "                                              predictionCol='prediction', \n",
    "                                              metricName='weightedPrecision') \n",
    "\n",
    "precision = evaluator.evaluate(predictions) \n",
    "print('Precision del test = ', precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e151c",
   "metadata": {},
   "source": [
    "**Actividad 4** (2,0 ptos)\n",
    "\n",
    "Calcular la predicción y la precisión del modelo empleando\n",
    "Gradient-boosted tree classifier\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "Gradient-boosted tree classifier solo soporta target binario, en este caso el target tiene 3 labels. Se cambia el modelo a utilizar por Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41d0d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision del test =  0.9345238095238095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/23 12:58:56 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/09/23 12:58:56 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "# importamos funciones necesarias\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "# inicializamos modele\n",
    "nb = NaiveBayes(labelCol=\"type\", featuresCol=\"numeric_fields\", \n",
    "                smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# entrenamos model\n",
    "model = nb.fit(df_train)\n",
    "\n",
    "# hacemos prediccion\n",
    "predictions = model.transform(df_test)\n",
    "\n",
    "# calculamos metrica\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='type', \n",
    "                                              predictionCol='prediction', \n",
    "                                              metricName='weightedPrecision') \n",
    "\n",
    "precision = evaluator.evaluate(predictions) \n",
    "print('Precision del test = ', precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7de1444",
   "metadata": {},
   "source": [
    "**Actividad 5** (2,0 ptos)\n",
    "\n",
    "Calcular la predicción y la precisión del modelo empleando Random\n",
    "Forest Classifier\n",
    "\n",
    "**Respuesta:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22f4d30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision del test =  1.0\n"
     ]
    }
   ],
   "source": [
    "# importamos funciones necesarias\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# inicializamos modele\n",
    "rf = RandomForestClassifier(labelCol=\"type\", featuresCol=\"numeric_fields\", numTrees=10)\n",
    "\n",
    "# entrenamos model\n",
    "model = rf.fit(df_train)\n",
    "\n",
    "# hacemos prediccion\n",
    "predictions = model.transform(df_test)\n",
    "\n",
    "# calculamos metrica\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='type', \n",
    "                                              predictionCol='prediction', \n",
    "                                              metricName='weightedPrecision') \n",
    "\n",
    "precision = evaluator.evaluate(predictions) \n",
    "print('Precision del test = ', precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
